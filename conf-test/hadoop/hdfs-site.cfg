# Basic
dfs.namenode.name.dir=file:///var/hadoop/data/dfsname
dfs.datanode.data.dir=file:///var/hadoop/data/dfsdata
dfs.replication=2
dfs.datanode.du.reserved=1073741824
dfs.blockreport.intervalMsec=600000
dfs.datanode.directoryscan.interval=600
dfs.namenode.datanode.registration.ip-hostname-check=false
dfs.hosts.exclude=/usr/hadoop/current/etc/hadoop/excludes

# NameNode HA
dfs.nameservices=cluster1
dfs.ha.namenodes.cluster1=nn1,nn2
dfs.namenode.rpc-address.cluster1.nn1=hdpc1-mn01:8020
dfs.namenode.rpc-address.cluster1.nn2=hdpc1-mn02:8020
dfs.namenode.http-address.cluster1.nn1=hdpc1-mn01:50070
dfs.namenode.http-address.cluster1.nn2=hdpc1-mn02:50070
dfs.namenode.shared.edits.dir=qjournal://hdpc1-sn001:8485;hdpc1-sn002:8485;hdpc1-sn003:8485/cluster1
dfs.client.failover.proxy.provider.cluster1=org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider
dfs.ha.fencing.methods=sshfence
dfs.ha.fencing.ssh.private-key-files=/home/hdfs/.ssh/id_rsa
dfs.ha.fencing.ssh.connect-timeout=30000
dfs.journalnode.edits.dir=/var/hadoop/data/journal
dfs.ha.automatic-failover.enabled=true
ha.zookeeper.session-timeout.ms=10000

# Tuning
dfs.blocksize=64m
dfs.namenode.handler.count=10
dfs.datanode.handler.count=10
dfs.datanode.max.transfer.threads=4096
dfs.datanode.balance.bandwidthPerSec=10485760
dfs.namenode.replication.work.multiplier.per.iteration=4
dfs.namenode.replication.max-streams=10
dfs.namenode.replication.max-streams-hard-limit=20