# Basic
dfs.namenode.name.dir=file:///work/hadoop/data/dfsname
dfs.datanode.data.dir=file:///work/hadoop/data/dfsdata
dfs.replication=3
dfs.datanode.du.reserved=10737418240
dfs.blockreport.intervalMsec=3600000
dfs.datanode.directoryscan.interval=3600
dfs.namenode.datanode.registration.ip-hostname-check=false
dfs.hosts.exclude=/usr/hadoop/current/etc/hadoop/excludes

# NameNode HA
dfs.nameservices=cluster1
dfs.ha.namenodes.cluster1=nn1,nn2
dfs.namenode.rpc-address.cluster1.nn1=yygz-99.tjinserv.com:8020
dfs.namenode.rpc-address.cluster1.nn2=yygz-101.tjinserv.com:8020
dfs.namenode.http-address.cluster1.nn1=yygz-99.tjinserv.com:50070
dfs.namenode.http-address.cluster1.nn2=yygz-101.tjinserv.com:50070
dfs.namenode.shared.edits.dir=qjournal://yygz-104.tjinserv.com:8485;yygz-110.tjinserv.com:8485;yygz-111.tjinserv.com:8485/cluster1
dfs.client.failover.proxy.provider.cluster1=org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider
dfs.ha.fencing.methods=sshfence
dfs.ha.fencing.ssh.private-key-files=/home/hdfs/.ssh/id_rsa
dfs.ha.fencing.ssh.connect-timeout=30000
dfs.journalnode.edits.dir=/work/hadoop/data/journal
dfs.ha.automatic-failover.enabled=true
ha.zookeeper.session-timeout.ms=10000

# Tuning
dfs.blocksize=128m
dfs.namenode.handler.count=160
dfs.datanode.handler.count=20
dfs.datanode.max.transfer.threads=10240
dfs.datanode.balance.bandwidthPerSec=10485760